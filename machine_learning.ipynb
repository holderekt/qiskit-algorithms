{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from skimage import transform\n",
    "\n",
    "\n",
    "from qiskit import QuantumRegister, ClassicalRegister, QuantumCircuit, transpile,assemble\n",
    "from qiskit import Aer, execute\n",
    "from qiskit.extensions import Initialize\n",
    "from qiskit.tools.visualization import plot_histogram, plot_bloch_multivector, array_to_latex\n",
    "from qiskit.quantum_info import partial_trace, Statevector, random_statevector, Operator, SparsePauliOp\n",
    "from qiskit_textbook.tools import simon_oracle\n",
    "\n",
    "from qiskit.circuit import QuantumCircuit, Parameter, ParameterVector\n",
    "from qiskit.circuit.library import PauliFeatureMap, ZFeatureMap, ZZFeatureMap\n",
    "from qiskit.circuit.library import TwoLocal, NLocal, RealAmplitudes, EfficientSU2\n",
    "from qiskit.circuit.library import HGate, RXGate, RYGate, RZGate, CXGate, CRXGate, CRZGate\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "\n",
    "from qiskit_machine_learning.kernels import QuantumKernel\n",
    "from qiskit_machine_learning.algorithms.classifiers import VQC, NeuralNetworkClassifier\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "\n",
    "from qiskit.algorithms.optimizers import COBYLA, GradientDescent, ADAM\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit.utils import algorithm_globals\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "algorithm_globals.random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bell_state(circuit, q1, q2):\n",
    "    circuit.h(q1)\n",
    "    circuit.cnot(q1, q2)\n",
    "\n",
    "def create_bell_state(circuit, register):\n",
    "    circuit.h(register[0])\n",
    "    circuit.cnot(register[0], register[1])\n",
    "\n",
    "def teleport_gates(circuit, alice_qbit, alice_ent):\n",
    "    circuit.cnot(alice_qbit, alice_ent)\n",
    "    circuit.h(alice_qbit)\n",
    "\n",
    "\n",
    "def swap(circuit, q1, q2):\n",
    "    circuit.cnot(q1, q2)\n",
    "    circuit.cnot(q2,q1)\n",
    "    circuit.cnot(q1, q2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis state\n",
    "\n",
    "Feature binarie. Dati i campioni assegnamo equo amplitude e nulla alle altre. Dato che \n",
    "\n",
    "$\n",
    "\\sum_{i=1}^{2^N} \\alpha_i^2 = 1\n",
    "$\n",
    "\n",
    "la somma delle amplitude per ogni stato deve essere uguale ad uno, e vogliamo assegnare equa amplitude (solo per i dati presenti), questa sarà (con N numero campioni):\n",
    "\n",
    "$\n",
    "1 = \\sum_{i=1}^{N} \\alpha^2 = N\\alpha^2  \\rightarrow \\alpha = \\frac{1}{\\sqrt{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 3\n",
    "DATA = ['101', '111', '000']\n",
    "\n",
    "state_preparation = [0] * (2**N_FEATURES)\n",
    "for e in DATA:\n",
    "    state_preparation[int(e, 2)] = 1/math.sqrt(len(DATA))\n",
    "\n",
    "print(state_preparation)\n",
    "\n",
    "qc = QuantumCircuit(N_FEATURES, N_FEATURES)\n",
    "qc.initialize(state_preparation, range(N_FEATURES))\n",
    "\n",
    "\n",
    "qc.decompose().draw(output=\"mpl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amplitude\n",
    "\n",
    "Per rappresentare un intero dataset concateno i dati di ogni elemento. Per un dataset in $\\mathbb{R}^{n\\times n}$ avrò quindi bisogno di   \n",
    "\n",
    "\n",
    "$\n",
    "N_{qbit} = n^2 log_2(n^2)\n",
    "$\n",
    "\n",
    "\n",
    "Dato quindi un dataset $X = (\\alpha_1, \\dots , \\alpha_n)$ necessario rinormalizzare i dati in modo tale che, data una costante di normalizzazione $k$:\n",
    "\n",
    "$\n",
    "\\sum_{i=1}^{n} |(k\\alpha_i)^2| = 1\n",
    "$\n",
    "si ha quindi\n",
    "\n",
    "$\n",
    "k = \\sqrt{\\frac{1} {\\sum_{i=1}^{n} |\\alpha_i^2|}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = np.array([[5,2,3],[1,3,1]])\n",
    "N_QBIT = math.ceil(math.log2(DATA.size))\n",
    "print(N_QBIT)\n",
    "value =  math.sqrt(1/(np.sum(DATA.flatten()**2)))\n",
    "\n",
    "state_preparation = np.append(DATA.flatten() * value, [0] * (2**N_QBIT - DATA.size))\n",
    "\n",
    "qc = QuantumCircuit(N_QBIT, N_QBIT)\n",
    "qc.initialize(state_preparation, range(N_QBIT))\n",
    "\n",
    "qc.decompose().draw(output=\"mpl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Inner product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits(n_class=2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6,3))\n",
    "axs[0].set_axis_off()\n",
    "axs[0].imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "axs[1].set_axis_off()\n",
    "axs[1].imshow(digits.images[2], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridimensionamento su 4 dimensioni e normalizzazione e standardizzazione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DIM = 4\n",
    "TRAIN_SIZE = 100\n",
    "TEST_SIZE = 20\n",
    "\n",
    "\n",
    "sample_train, sample_test, label_train, label_test = train_test_split(\n",
    "    digits.data, \n",
    "    digits.target, \n",
    "    test_size=0.2, \n",
    "    random_state=22\n",
    "    )\n",
    "    \n",
    "\n",
    "pca = PCA(n_components=N_DIM).fit(sample_train)\n",
    "sample_train = pca.transform(sample_train)\n",
    "sample_test = pca.transform(sample_test)\n",
    "\n",
    "std_scale = StandardScaler().fit(TRAIN)\n",
    "sample_train = std_scale.transform(sample_train)\n",
    "sample_test = std_scale.transform(sample_test)\n",
    "\n",
    "samples = np.append(sample_train, sample_test, axis=0)\n",
    "minmax_scale = MinMaxScaler((-1, 1)).fit(samples)\n",
    "sample_train = minmax_scale.transform(sample_train)\n",
    "sample_test = minmax_scale.transform(sample_test)\n",
    "\n",
    "\n",
    "sample_train = sample_train[:TRAIN_SIZE]\n",
    "label_train = label_train[:TRAIN_SIZE]\n",
    "\n",
    "sample_test = sample_test[:TEST_SIZE]\n",
    "label_test = label_test[:TEST_SIZE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene mostrata la mappa di encoding dei dati (non verrà usata questa nelle sezioni successive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_map = ZZFeatureMap(feature_dimension=N_DIM, reps=1, entanglement='linear', insert_barriers=True)\n",
    "encode_circuit = encode_map.bind_parameters(sample_train[0])\n",
    "encode_circuit.decompose().draw(output='mpl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definita la feature map ZZ per l'encoding dei dati, creiamo il quantum kernel sulla base di questa feature map. Il quantum kernel di occuperà di definire un circuito che effettua l'inner product della feature map per la sua coniugata trasposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz_map = ZZFeatureMap(feature_dimension=4, reps=2, entanglement='linear', insert_barriers=True)\n",
    "zz_kernel = QuantumKernel(feature_map=zz_map, quantum_instance=Aer.get_backend('statevector_simulator'))\n",
    "\n",
    "zz_circuit = zz_kernel.construct_circuit(sample_train[6], sample_train[7])\n",
    "zz_circuit.decompose().decompose().draw(output='mpl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolata la feature map e il kernel, possiamo facilemente visualizzare la correlazione tra due istanze del dataset. Se due vettori sono fortemente correlati dato in input il basis state 0 mi aspetto di trovare in output il basis state 0 (un numero maggiore di volte). Abbiamo infatti due gate hadamard che riportano lo stato dalla equisuperposition ad una superposition. Se sono correlati la trasformazione delle feature map non modificherà troppo le amplitude e produrrà lo stesso output. Prendiamo infatti come similarità (espressa come probabilità). La frequenza di output dello stato 0. Indici di esempio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_A = 0\n",
    "ID_B = 10\n",
    "\n",
    "zz_map = ZZFeatureMap(feature_dimension=4, reps=1, entanglement='linear', insert_barriers=True)\n",
    "zz_kernel = QuantumKernel(feature_map=zz_map, quantum_instance=Aer.get_backend('statevector_simulator'))\n",
    "\n",
    "zz_circuit = zz_kernel.construct_circuit(sample_train[ID_A], sample_train[ID_B])\n",
    "zz_circuit.decompose().decompose().draw(output='mpl')\n",
    "\n",
    "backend = Aer.get_backend('qasm_simulator')\n",
    "job = execute(zz_circuit, backend, seed_transpiler=1024)\n",
    "counts = job.result().get_counts(zz_circuit)\n",
    "\n",
    "print(f\"{'A:':<5s}{sample_train[ID_A]}\")\n",
    "print(f\"{'B:':<5s}{sample_train[ID_B]}\")\n",
    "print(f\"{'A~B:':<5s}{(counts['0'*N_DIM]/sum(counts.values())) * 100:04.2f}%\")\n",
    "\n",
    "plot_histogram(counts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene calcolata quindi la matrice kernel sulla base delle similarità calcolate come nel codice precedente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_train = zz_kernel.evaluate(x_vec=sample_train) \n",
    "matrix_test = zz_kernel.evaluate(x_vec=sample_test, y_vec=sample_train)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(np.asmatrix(matrix_train), interpolation='nearest', origin='upper', cmap='Blues')\n",
    "axs[0].set_title(\"Training Kernel Matrix\")\n",
    "axs[1].imshow(np.asmatrix(matrix_test), interpolation='nearest', origin='upper', cmap='Reds')\n",
    "axs[1].set_title(\"Testing Kernel Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training e testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantum_svm = SVC(kernel=zz_kernel.evaluate)\n",
    "quantum_svm.fit(sample_train, label_train)\n",
    "quantum_svm_score = quantum_svm.score(sample_test, label_test)\n",
    "print(f\"Quantum SVM (ZZ Encoding) score: {quantum_svm_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantum_svm = SVC(kernel='precomputed')\n",
    "quantum_svm.fit(matrix_train, label_train)\n",
    "quantum_svm_score = quantum_svm.score(matrix_test, label_test)\n",
    "print(f\"Quantum SVM (ZZ Encoding) score: {quantum_svm_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Creazione del dataset, feature randomiche con valori tra -1 e +1, classe 0 se la somma è minore di 0, classe 1 se la somma è maggiore di 0\n",
    "La rete utilizzata utilizza:\n",
    "\n",
    "- COBYLA come ottimizzatore\n",
    "- RealAmplitude come encoder (amplitude encoding) e implementazione del circuito variazionale (ansatz, circuito parametrizzato, implementazione della rete neurale nel nostro caso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 40\n",
    "NUM_FEATURES  = 2\n",
    "TRAIN_SIZE = 30\n",
    "\n",
    "DATA = 2 * algorithm_globals.random.random([DATA_SIZE, NUM_FEATURES]) - 1\n",
    "LABELS = 1 * (np.sum(DATA, axis=1) > 0)\n",
    "\n",
    "DATA = MinMaxScaler().fit_transform(DATA)\n",
    "LABELS = OneHotEncoder(sparse=False).fit_transform(LABELS.reshape(-1,1))\n",
    "\n",
    "print(DATA.shape)\n",
    "print(LABELS.shape)\n",
    "print(DATA[0:5, :],'\\n',LABELS[0:5, :])\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(DATA, LABELS, train_size=TRAIN_SIZE, random_state=algorithm_globals.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTIVE_VALUES = []\n",
    "IT = 100\n",
    "def callback_graph(_, objective_value):\n",
    "    clear_output(wait=True)\n",
    "    OBJECTIVE_VALUES.append(objective_value)\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    stage1_len = np.min((len(OBJECTIVE_VALUES), IT))\n",
    "    stage1_x = np.linspace(1, stage1_len, stage1_len)\n",
    "    stage1_y = OBJECTIVE_VALUES[:stage1_len]\n",
    "    plt.plot(stage1_x, stage1_y, color=\"orange\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTIVE_VALUES = []\n",
    "optimizer = COBYLA(maxiter=IT)\n",
    "ansatz = RealAmplitudes(NUM_FEATURES)\n",
    "model = VQC(ansatz=ansatz, optimizer=optimizer, initial_point=[0.5] * ansatz.num_parameters, callback=callback_graph)\n",
    "model.fit(train_data, train_labels)\n",
    "\n",
    "print(\"Train score : \", model.score(train_data, train_labels))\n",
    "print(\"Test score  : \", model.score(test_data, test_labels))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv e Pool Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer convoluzione con $N(\\alpha, \\beta, \\gamma) \\subset SU(n)$ (Special Unitary Groups of Degree n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_circuit(params):\n",
    "    target = QuantumCircuit(2)\n",
    "    target.rz(-np.pi / 2, 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(params[0], 0)\n",
    "    target.ry(params[1], 1)\n",
    "    target.cx(0, 1)\n",
    "    target.ry(params[2], 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(np.pi / 2, 0)\n",
    "    return target\n",
    "\n",
    "\n",
    "def pool_circuit(params):\n",
    "    target = QuantumCircuit(2)\n",
    "    target.rz(-np.pi / 2, 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(params[0], 0)\n",
    "    target.ry(params[1], 1)\n",
    "    target.cx(0, 1)\n",
    "    target.ry(params[2], 1)\n",
    "    return target\n",
    "\n",
    "\n",
    "def conv_layer(n_qbit, params_prefix, name=\"conv_layer\"):\n",
    "\n",
    "    qc = QuantumCircuit(n_qbit, name=name)\n",
    "    qbits = list(range(n_qbit))\n",
    "    params = ParameterVector(params_prefix, n_qbit * 3)\n",
    "    \n",
    "    if n_qbit > 2:\n",
    "        for index, couple in enumerate(zip(qbits, qbits[1:] + [0])):\n",
    "            qc = qc.compose(conv_circuit(params[index*3 : (index*3)+3]), [couple[0],couple[1]])\n",
    "            qc.barrier()\n",
    "    else:\n",
    "        qc = qc.compose(conv_circuit(params[0 : 3]), [0,1])\n",
    "        qc.barrier()\n",
    "\n",
    "    qc_inst = qc.to_instruction()\n",
    "    qc = QuantumCircuit(n_qbit)\n",
    "    qc.append(qc_inst, qbits)\n",
    "\n",
    "    return qc\n",
    "\n",
    "\n",
    "def pool_layer(sources, sinks, params_prefix, name=\"pool_layer\"):\n",
    "    n_qbit = len(sources) + len(sinks)\n",
    "    qc = QuantumCircuit(n_qbit, name=name)\n",
    "    params = ParameterVector(params_prefix, n_qbit // 2 * 3)\n",
    "\n",
    "    for index, couple in enumerate(zip(sources, sinks)):\n",
    "        qc = qc.compose(pool_circuit(params[index*3 : (index*3) + 3]), [couple[0], couple[1]])\n",
    "        qc.barrier()\n",
    "\n",
    "    qc_inst = qc.to_instruction()\n",
    "    qc = QuantumCircuit(n_qbit)\n",
    "    qc.append(qc_inst, range(n_qbit))\n",
    "\n",
    "    return qc\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_QBIT = 4\n",
    "\n",
    "print(\"CONV CIRCUIT\")\n",
    "display(conv_circuit([1,1,1]).draw(output=\"mpl\"))\n",
    "print(\"POOL CIRCUIT\")\n",
    "display(pool_circuit([1,1,1]).draw(output=\"mpl\"))\n",
    "print(\"CONV LAYER\")\n",
    "display(conv_layer(N_QBIT, '\\u03B8').draw(output=\"mpl\"))\n",
    "display(conv_layer(N_QBIT, '\\u03B8').decompose().draw(output=\"mpl\"))\n",
    "\n",
    "\n",
    "print(\"POOL LAYER\")\n",
    "display(pool_layer([0,1], [2,3], '\\u03B8').draw(output=\"mpl\"))\n",
    "display(pool_layer([0,1], [2,3], '\\u03B8').decompose().draw(output=\"mpl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits(n_class=2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6,3))\n",
    "axs[0].set_axis_off()\n",
    "axs[0].imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "axs[1].set_axis_off()\n",
    "axs[1].imshow(digits.images[2], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "resize_data = [transform.resize(x, (6,6), mode='constant').ravel() for x in digits.images]\n",
    "\n",
    "plt.imshow(resize_data[0].reshape((6,6)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 2\n",
    "N_TRAIN = 20\n",
    "STANDARDIZE = False\n",
    "MINMAXSCALE = False\n",
    "\n",
    "digits = datasets.load_digits(n_class=N_CLASSES)\n",
    "\n",
    "resize_data = np.array([transform.resize(x, (4,4), mode='constant').ravel() for x in digits.images])\n",
    "\n",
    "\n",
    "TRAIN, TEST, TRAIN_LABELS, TEST_LABELS = train_test_split(\n",
    "    resize_data, \n",
    "    np.array([1 if x else -1 for x in digits.target]), \n",
    "    test_size=0.3, \n",
    "    random_state=22\n",
    "    )\n",
    "\n",
    "    \n",
    "if STANDARDIZE:\n",
    "    std_scale = StandardScaler().fit(TRAIN)\n",
    "    TRAIN = std_scale.transform(TRAIN)\n",
    "    TEST = std_scale.transform(TEST)\n",
    "\n",
    "if MINMAXSCALE:\n",
    "    samples = np.append(TRAIN, TEST, axis=0)\n",
    "    minmax_scale = MinMaxScaler((-1, 1)).fit(samples)\n",
    "    TRAIN = minmax_scale.transform(TRAIN)\n",
    "    TEST = minmax_scale.transform(TEST)\n",
    "\n",
    "\n",
    "TRAIN = TRAIN[:N_TRAIN,:]\n",
    "TRAIN_LABELS = TRAIN_LABELS[:N_TRAIN]\n",
    "\n",
    "\n",
    "TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_images):\n",
    "    images = []\n",
    "    labels = []\n",
    "    hor_array = np.zeros((6, 8))\n",
    "    ver_array = np.zeros((4, 8))\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0, 7):\n",
    "        if i != 3:\n",
    "            hor_array[j][i] = np.pi / 2\n",
    "            hor_array[j][i + 1] = np.pi / 2\n",
    "            j += 1\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0, 4):\n",
    "        ver_array[j][i] = np.pi / 2\n",
    "        ver_array[j][i + 4] = np.pi / 2\n",
    "        j += 1\n",
    "\n",
    "    for n in range(num_images):\n",
    "        rng = algorithm_globals.random.integers(0, 2)\n",
    "        if rng == 0:\n",
    "            labels.append(-1)\n",
    "            random_image = algorithm_globals.random.integers(0, 6)\n",
    "            images.append(np.array(hor_array[random_image]))\n",
    "        elif rng == 1:\n",
    "            labels.append(1)\n",
    "            random_image = algorithm_globals.random.integers(0, 4)\n",
    "            images.append(np.array(ver_array[random_image]))\n",
    "\n",
    "        # Create noise\n",
    "        for i in range(8):\n",
    "            if images[-1][i] == 0:\n",
    "                images[-1][i] = algorithm_globals.random.uniform(0, np.pi / 4)\n",
    "    return images, labels\n",
    "\n",
    "images, labels = generate_dataset(50)\n",
    "\n",
    "TRAIN, TEST, TRAIN_LABELS, TEST_LABELS = train_test_split(\n",
    "    images, labels, test_size=0.3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'DATA SHAPE':15s}: {np.array(TRAIN).shape}\")\n",
    "\n",
    "N_QBIT = np.array(TRAIN).shape[1]\n",
    "\n",
    "feature_map = ZFeatureMap(N_QBIT)\n",
    "ansatz = QuantumCircuit(N_QBIT, name=\"Ansatz\")\n",
    "\n",
    "current_qbit = N_QBIT\n",
    "index = 1\n",
    "\n",
    "while(current_qbit >= 2):\n",
    "    ansatz.compose(conv_layer(current_qbit, f'c{index}'), list(range(N_QBIT - current_qbit, N_QBIT)), inplace=True)\n",
    "    ansatz.compose(pool_layer(list(range(0, current_qbit // 2)), list(range(current_qbit // 2 , current_qbit)), f'p{index}'), list(range(N_QBIT - current_qbit, N_QBIT)), inplace=True)\n",
    "    current_qbit = current_qbit // 2\n",
    "    index += 1\n",
    "\n",
    "\n",
    "circuit = QuantumCircuit(N_QBIT)\n",
    "circuit.compose(feature_map, range(N_QBIT), inplace=True)\n",
    "circuit.compose(ansatz, range(N_QBIT), inplace=True)\n",
    "\n",
    "print(\"\\nQCNN Created:\")\n",
    "print(f\"{'Input':15s}: {N_QBIT}\")\n",
    "print(f\"{'Depth':15s}: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz.draw(output=\"mpl\", filename=\"circuit.png\")\n",
    "ansatz.decompose().draw(output=\"mpl\", filename=\"circuit_decomposed.png\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_print(_, objective_value):\n",
    "    global losses\n",
    "    losses.append(objective_value)\n",
    "    #print(f\"Epoch: [{len(losses):3d}]  Losss: {objective_value:4.3f}\")\n",
    "    \n",
    "observable = SparsePauliOp.from_list([(\"Z\" + \"I\" * (N_QBIT - 1), 1)])\n",
    "\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=circuit.decompose(),\n",
    "    observables=observable,\n",
    "    input_params=feature_map.parameters,\n",
    "    weight_params=ansatz.parameters,\n",
    ")\n",
    "\n",
    "initial_point = np.asarray([0.5] * ansatz.num_parameters)\n",
    "\n",
    "classifier = NeuralNetworkClassifier(\n",
    "    qnn,\n",
    "    optimizer=COBYLA(maxiter=400),\n",
    "    callback=callback_print,\n",
    "    initial_point=initial_point,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(TRAIN)\n",
    "y = np.asarray(TRAIN_LABELS)\n",
    "\n",
    "global losses\n",
    "losses = []\n",
    "classifier.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,len(losses)+1)), losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"QCNN Training\")\n",
    "print(f\"Accuracy from the train data : {np.round(100 * classifier.score(x, y), 2)}%\")\n",
    "y_predict = classifier.predict(TEST)\n",
    "tx = np.asarray(TEST)\n",
    "ty = np.asarray(TEST_LABELS)\n",
    "print(f\"Accuracy from the test data : {np.round(100 * classifier.score(tx, ty), 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad11a8bee41c5e9e49935bcfc057bf24c040f5e4c134dd6ff301c88f66bc37e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
