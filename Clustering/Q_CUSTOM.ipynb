{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "from qiskit import ClassicalRegister, QuantumCircuit, QuantumRegister\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from qiskit import Aer, execute\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_encoding(x):\n",
    "    N_QBIT = math.ceil(math.log2(x.size))\n",
    "    value =  math.sqrt(1/(np.sum(x.flatten()**2)))\n",
    "    state_preparation = np.append(x.flatten() * value, [0] * (2**N_QBIT - x.size))\n",
    "\n",
    "    qc = QuantumCircuit(N_QBIT, name='Encoding')\n",
    "    qc.initialize(state_preparation, range(N_QBIT))\n",
    "\n",
    "    return qc\n",
    "\n",
    "def angles_encoding(x, rotation='y'):\n",
    "    qc = QuantumCircuit(len(x), name='Encoding')\n",
    "    rot_fun = None\n",
    "    if rotation == 'y':\n",
    "        rot_fun = qc.ry\n",
    "    elif rotation == 'x':\n",
    "        rot_fun = qc.rx\n",
    "    else:\n",
    "        rot_fun = qc.rz\n",
    "\n",
    "    for i, el in enumerate(x):\n",
    "        rot_fun(el, i)\n",
    "        \n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 100\n",
    "N_CLUSTER = 4\n",
    "N_FEATURES = 4\n",
    "\n",
    "dataset, classes = datasets.make_blobs(n_samples=TRAIN_SIZE, random_state=10, centers= N_CLUSTER, n_features= N_FEATURES,)\n",
    "dataset = np.array(dataset)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "mean = np.mean(dataset, axis = 0)\n",
    "std = np.std(dataset, axis = 0)\n",
    "centers = np.random.randn(N_CLUSTER, N_FEATURES)*std + mean\n",
    "\n",
    "plt.scatter(dataset[:,0], dataset[:,1], c=classes, cmap=\"Accent\")\n",
    "plt.scatter(centers[:,0], centers[:,1], marker='*', c='r', s=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circ = angles_encoding(np.append(dataset[0], centers[0]))\n",
    "circ.decompose().decompose().decompose().decompose().decompose().decompose().draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def point_centroid_distance(point, center, encoding):\n",
    "\n",
    "    encoded_data = encoding(np.append(point, center))\n",
    "    data = QuantumRegister(encoded_data.num_qubits, 'data')\n",
    "    ancilla = QuantumRegister(1, 'ancilla')\n",
    "    classical = ClassicalRegister(1, 'output')\n",
    "    qc = QuantumCircuit(data, ancilla, classical)\n",
    "\n",
    "    qc.h(range(qc.num_qubits))\n",
    "\n",
    "    qc.append(encoded_data.decompose().to_instruction(), data)\n",
    "\n",
    "    qc.barrier()\n",
    "\n",
    "    for i in range(int(encoded_data.num_qubits / 2)):\n",
    "        qc.cswap(ancilla, i, i+ int(encoded_data.num_qubits / 2))\n",
    "\n",
    "    qc.barrier()\n",
    "\n",
    "    qc.h(ancilla)\n",
    "    qc.measure(ancilla, classical)\n",
    "\n",
    "    #display(qc.draw(output='mpl'))\n",
    "\n",
    "    backend = Aer.get_backend('qasm_simulator')\n",
    "    job = execute(qc, backend=backend, shots=5000)\n",
    "    result = job.result().get_counts(qc)\n",
    "\n",
    "    return result.get('1', 0)\n",
    "\n",
    "\n",
    "def closest_centroid(points, centroids, encoding):\n",
    "    closest = []\n",
    "    for point in points:\n",
    "        closest.append(np.argmin([point_centroid_distance(point, c, encoding) for c in centroids]), )\n",
    "    return closest\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = closest_centroid(dataset, centers, angles_encoding)\n",
    "plt.scatter(dataset[:,0], dataset[:,1], c=values, cmap=\"Accent\")\n",
    "plt.scatter(centers[:,0], centers[:,1], marker='*', c=np.array(range(N_CLUSTER)), s=150, cmap=\"Accent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "ERROR_TRESHOLD = 0.01\n",
    "\n",
    "current_error = math.inf\n",
    "old_error = 0\n",
    "centers_new = centers\n",
    "iter = 1\n",
    "\n",
    "while abs(old_error - current_error) > ERROR_TRESHOLD:\n",
    "    \n",
    "    clusters = closest_centroid(dataset, centers_new, angles_encoding)\n",
    "\n",
    "    centers_old = deepcopy(centers_new)\n",
    "    centers_new = np.array([np.mean(dataset[np.equal(clusters, i)], axis=0) for i in range(N_CLUSTER)])\n",
    "\n",
    "    plt.title(iter)\n",
    "    plt.scatter(dataset[:,0], dataset[:,1], c=clusters, cmap=\"Accent\")\n",
    "    plt.scatter(centers_old[:,0], centers_old[:,1], marker='*', c=np.array(range(N_CLUSTER)), s=150, cmap=\"Accent\")\n",
    "    plt.scatter(centers_new[:,0], centers_new[:,1], marker='*', c='r', s=150)\n",
    "    plt.show()\n",
    "\n",
    "    iter += 1\n",
    "\n",
    "    old_error = deepcopy(current_error)\n",
    "    current_error = np.linalg.norm(centers_new - centers_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction = closest_centroid(dataset, centers_new, angles_encoding)\n",
    "\n",
    "plt.title(\"Final model\")\n",
    "plt.scatter(dataset[:,0], dataset[:,1], c=prediction, cmap=\"Accent\")\n",
    "plt.scatter(centers_new[:,0], centers_new[:,1], marker='*', c=np.array(range(N_CLUSTER)), s=150, cmap=\"Accent\")\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
    "\n",
    "purity_score(classes, prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad11a8bee41c5e9e49935bcfc057bf24c040f5e4c134dd6ff301c88f66bc37e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
